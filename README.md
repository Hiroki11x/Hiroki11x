### About

I am a Ph.D. candidate in Computer Science at [Université de Montréal](https://www.umontreal.ca/) and [Mila - Quebec AI Institute](https://mila.quebec/en/), advised by [Ioannis Mitliagkas](http://mitliagkas.github.io/).

My research focuses on large-scale optimization and distributed training for deep learning, with an emphasis on high-performance computing and the efficient training of large language models. I am particularly interested in semi-synchronous and large-batch training regimes, including critical batch size analysis, as well as non-Euclidean optimization and the design of scalable optimization algorithms.

Previously, I worked on out-of-distribution generalization and confidence calibration, and investigated optimization dynamics in generative models. More recently, my research has expanded to efficient fine-tuning and fairness in large language models, bridging theoretical insights with practical large-scale systems.

I most recently completed a research internship at [Meta Superintelligence Lab - Infra (Menlo Park)](https://aisystemcodesign.github.io/), working on large-batch training and optimization for foundation models. I have also been a Student Researcher at [Google DeepMind (Mountain View)](https://deepmind.google/) and a research intern at [Microsoft Research (Redmond)](https://www.microsoft.com/en-us/research/about-microsoft-research/).

I am a recipient of the [Masason Foundation Fellowship](https://masason-foundation.org/en/) and the [RBC Borealis Fellowship](https://rbcborealis.com/news/the-2024-2025-rbc-borealis-fellows-driving-the-future-of-ai/). I received my B.Sc. (2017) and M.Sc. (2019) from the [Tokyo Institute of Technology](https://www.titech.ac.jp/english), graduating as Valedictorian.

[CV (Jan 2026)](https://hiroki11x.github.io/files/CV_HirokiNAGANUMA.pdf) /  [Résumé (Jan 2026)](https://hiroki11x.github.io/files/Resume_HirokiNAGANUMA.pdf) / [Biography](https://hiroki11x.github.io/bio)
